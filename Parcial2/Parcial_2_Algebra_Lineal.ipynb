{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuRzLktGdHFbzP3WxcIcVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaiderSantos/metodos1_JaiderSantos_NicolasGomez/blob/main/Parcial2/Parcial_2_Algebra_Lineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YJgAcUU1Ezcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sym\n",
        "import numpy as np\n",
        "from scipy import integrate\n",
        "import math as math\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import roots_legendre\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import copy\n",
        "\n",
        "C = np.array([8.51, 10.68, 12.24, 13.66, 15.37, 17.15, 19.66, 24.69])\n",
        "Ca = 1000000\n",
        "\n",
        "\n",
        "def func_de_retorno(C, w, Ca):\n",
        "\n",
        "    w = np.resize(w, C.shape)\n",
        "    Pro_elem = w * C\n",
        "    menos1 = Pro_elem - 1\n",
        "    RTA = np.min(Ca * (menos1))\n",
        "    return RTA\n",
        "\n",
        "\n",
        "def func_de_retorno_max(C, w, Ca):\n",
        "\n",
        "    w = np.resize(w, C.shape)\n",
        "    Pro_elem = w * C\n",
        "    menos1 = Pro_elem - 1\n",
        "    RTA = Ca * np.max(menos1)\n",
        "    return RTA\n",
        "\n",
        "\n",
        "class Robot:\n",
        "    def __init__(self, f, Id, T):\n",
        "        self.Id = Id\n",
        "        self.f = f\n",
        "        self.h = np.zeros(T)\n",
        "        for i in range(T):\n",
        "            self.h[i] = np.random.uniform(1.0, 100.0)\n",
        "        self.h /= np.sum(self.h)\n",
        "\n",
        "        self.r = np.random.rand(len(C))\n",
        "        self.rate = 0.05\n",
        "\n",
        "    def Mutate(self):\n",
        "        self.r += np.random.normal(loc=0., scale=self.rate, size=len(self.r))\n",
        "\n",
        "    def GetR(self):\n",
        "        return self.r\n",
        "\n",
        "    def SetFitness(self):\n",
        "        self.Fitness = self.f(C, self.GetR(), Ca)  #\n",
        "\n",
        "\n",
        "        if self.Fitness > 0.05:\n",
        "            self.rate = 0.005\n",
        "\n",
        "    def GetFitness(self):\n",
        "        return self.Fitness\n",
        "\n",
        "\n",
        "def GetRobots(N, tam):\n",
        "    Robots = []\n",
        "\n",
        "    for i in range(N):\n",
        "        r = Robot(func_de_retorno, i, tam)\n",
        "        Robots.append(r)\n",
        "\n",
        "    return Robots\n",
        "\n",
        "\n",
        "Robots = np.array(GetRobots(500, len(C)))\n",
        "\n",
        "\n",
        "def GetRobots(N, tam):\n",
        "    Robots = []\n",
        "\n",
        "    for i in range(N):\n",
        "        r = Robot(func_de_retorno, i, tam)\n",
        "        Robots.append(r)\n",
        "\n",
        "    return Robots\n",
        "\n",
        "\n",
        "Robots = np.array(GetRobots(50, len(C)))\n",
        "\n",
        "for i in range(len(Robots)):\n",
        "    print(Robots[i].h)\n",
        "\n",
        "\n",
        "\n",
        "def Genetic(Robots, epochs = 500):\n",
        "\n",
        "\n",
        "\n",
        "    N = int(0.5*len(Robots))\n",
        "\n",
        "    Fitness = np.array([])\n",
        "\n",
        "    for e in range(int(epochs)):\n",
        "\n",
        "\n",
        "        for i,p in enumerate(Robots):\n",
        "            p.Mutate()\n",
        "            p.SetFitness()\n",
        "\n",
        "\n",
        "        scores = [ (p.Fitness, p) for p in Robots ]\n",
        "        scores.sort(key = lambda x: x[0], reverse = False)\n",
        "\n",
        "        o\n",
        "        Temp = [r[1] for i,r in enumerate(scores) if i < N ]\n",
        "\n",
        "        for i,r in enumerate(Robots):\n",
        "            j = i%N\n",
        "            Robots[i] = copy.deepcopy(Temp[j])\n",
        "\n",
        "\n",
        "\n",
        "        Fitness_ = [  r.GetFitness() for i,r in enumerate(Robots) ]\n",
        "\n",
        "       o\n",
        "        Fitness = np.append(Fitness,np.mean(Fitness_))\n",
        "\n",
        "        if Fitness[-1] < 0.0001:\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "        if e % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            ax,ax1 = Plotter(e)\n",
        "            for i,p in enumerate(Robots):\n",
        "                ax.scatter(p.GetR()[0],p.GetR()[1],marker='.',color='r',label='Id {}'.format(p.Id))\n",
        "            ax.set_xlim(-5,5)\n",
        "            ax.set_ylim(-5,5)\n",
        "\n",
        "\n",
        "            ax1.set_title('Fitness function')\n",
        "            ax1.plot(Fitness,color='k',label='Id {}, {:.2f}, {:.2f}, {:.2f}'.format(Robots[0].Id, Robots[0].GetR()[0],Robots[0].GetR()[1],Robots[0].GetR()[2]))\n",
        "            ax1.set_xlabel('Epoch')\n",
        "            ax1.legend(loc=1)\n",
        "\n",
        "\n",
        "Genetic(Robots)\n",
        "\n",
        "Pesos = np.array([0.185, 0.152, 0.137, 0.125, 0.116, 0.107, 0.096, 0.082])\n",
        "print(Pesos)\n",
        "print(Pesos.sum())\n",
        "params = np.array([ p.GetR() for i, p in enumerate(Robots)]).mean(axis=0)\n",
        "\n",
        "print(params)\n",
        "\n",
        "# punto g\n",
        "\n",
        "\n",
        "# ganacia minima\n",
        "min_ganancia = func_de_retorno(C, Pesos, Ca)\n",
        "print(\"En el peor de los casos el retorno seriade\",min_ganancia)\n",
        "#pocible maximo del retorno de inverciÃ³n\n",
        "max_ganancia=func_de_retorno_max(C,Pesos,Ca)\n",
        "print(\"el retorno maximo posible seria de\",max_ganancia)\n",
        "\n",
        "#cuanto se invierte en cada opcion\n",
        "invercion=np.zeros(len(Pesos))\n",
        "for i in range(len(Pesos)):\n",
        "  invercion[i]=Pesos[i]*Ca\n",
        "print(invercion)\n",
        "Total=np.sum(invercion)\n",
        "print(Total)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl0aAEGhE1b0",
        "outputId": "fa3521d2-ad2e-47c5-b8c5-4c6047c25dfa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.16334656 0.10380849 0.0820572  0.2139332  0.14725345 0.20129264\n",
            " 0.08599812 0.00231034]\n",
            "[0.13717006 0.09220262 0.16276751 0.13549772 0.11040624 0.14329661\n",
            " 0.12514868 0.09351056]\n",
            "[0.1628561  0.12267866 0.18098145 0.13863474 0.15066212 0.03791646\n",
            " 0.03207442 0.17419606]\n",
            "[0.12966773 0.0871576  0.16315453 0.18332757 0.12081459 0.14328247\n",
            " 0.08985066 0.08274486]\n",
            "[0.18698478 0.15776571 0.06682141 0.03806419 0.06497695 0.22411468\n",
            " 0.14611961 0.11515267]\n",
            "[0.20307067 0.2451051  0.20409976 0.04012483 0.11021896 0.0717509\n",
            " 0.04167489 0.08395488]\n",
            "[0.03496083 0.28222751 0.13881781 0.01804419 0.3638158  0.06636394\n",
            " 0.02746849 0.06830143]\n",
            "[0.13161517 0.07480188 0.00456382 0.21544375 0.08678346 0.15945759\n",
            " 0.16986551 0.15746882]\n",
            "[0.06041272 0.04352038 0.03422437 0.23101122 0.19749671 0.12949576\n",
            " 0.25176447 0.05207436]\n",
            "[0.14057188 0.16741709 0.160964   0.11977377 0.06387243 0.13074445\n",
            " 0.15482831 0.06182807]\n",
            "[0.24464519 0.03685661 0.05052824 0.19553056 0.16313752 0.06569291\n",
            " 0.22307232 0.02053664]\n",
            "[0.08262962 0.10434804 0.21625966 0.18456437 0.09923733 0.0476899\n",
            " 0.10320493 0.16206615]\n",
            "[0.24113179 0.21594598 0.15287642 0.0351592  0.03465599 0.11553809\n",
            " 0.14256235 0.06213019]\n",
            "[0.13190802 0.148279   0.0106985  0.08686044 0.22824726 0.18728722\n",
            " 0.03193016 0.1747894 ]\n",
            "[0.2080062  0.24263396 0.01456968 0.02441422 0.20778768 0.05986342\n",
            " 0.19104443 0.05168042]\n",
            "[0.10112532 0.15117086 0.15935117 0.19379142 0.10463157 0.10446357\n",
            " 0.01349461 0.17197147]\n",
            "[0.09301297 0.07913986 0.13587043 0.05203925 0.080613   0.01976947\n",
            " 0.30177724 0.23777778]\n",
            "[0.23861593 0.11352036 0.10783194 0.03179143 0.03293206 0.01408388\n",
            " 0.15801937 0.30320503]\n",
            "[0.09951434 0.18942072 0.09193979 0.21012325 0.02181355 0.03772728\n",
            " 0.03252953 0.31693154]\n",
            "[0.18387224 0.12369925 0.11583797 0.23471691 0.02207872 0.06414433\n",
            " 0.08346746 0.17218313]\n",
            "[0.027297   0.07241256 0.06989829 0.03319847 0.0311455  0.13388083\n",
            " 0.32827705 0.30389031]\n",
            "[0.2245333  0.05439551 0.1361219  0.02198279 0.17078772 0.16744816\n",
            " 0.02881074 0.19591988]\n",
            "[0.09623001 0.0497111  0.08458469 0.20208719 0.13741308 0.12223006\n",
            " 0.09748818 0.21025569]\n",
            "[0.15217922 0.16732943 0.24002858 0.04767895 0.07901192 0.05655407\n",
            " 0.21758557 0.03963227]\n",
            "[0.13588752 0.11643899 0.08846285 0.05690618 0.14307508 0.17971488\n",
            " 0.13696753 0.14254697]\n",
            "[0.01748772 0.17550809 0.09045353 0.18667392 0.04446713 0.17303079\n",
            " 0.15592959 0.15644923]\n",
            "[0.06168017 0.24416357 0.22400444 0.08152465 0.26728946 0.05359025\n",
            " 0.06077905 0.00696841]\n",
            "[0.10158548 0.18631066 0.26491068 0.13139895 0.02017194 0.17142516\n",
            " 0.09210949 0.03208763]\n",
            "[0.11626096 0.17527025 0.14992365 0.19807248 0.09936368 0.17362064\n",
            " 0.01758984 0.06989849]\n",
            "[0.20758536 0.17671375 0.03616312 0.03970847 0.21821417 0.22031091\n",
            " 0.02566565 0.07563858]\n",
            "[0.03390322 0.23007657 0.18663921 0.07513981 0.18861718 0.16015193\n",
            " 0.10821769 0.01725439]\n",
            "[0.14374228 0.11994408 0.09773195 0.23821955 0.22854028 0.04156203\n",
            " 0.08539132 0.04486852]\n",
            "[0.08539619 0.12847552 0.06729993 0.15589813 0.14689956 0.13594886\n",
            " 0.15173696 0.12834484]\n",
            "[0.08964765 0.02137371 0.06556886 0.32778406 0.28060545 0.01550697\n",
            " 0.08501249 0.1145008 ]\n",
            "[0.08535878 0.13173258 0.04217092 0.1467012  0.0989924  0.20289654\n",
            " 0.1336893  0.15845827]\n",
            "[0.11635126 0.18260892 0.17076211 0.10837571 0.04459067 0.06043942\n",
            " 0.15668161 0.1601903 ]\n",
            "[0.22457271 0.06320484 0.05610478 0.06045469 0.28579911 0.17511032\n",
            " 0.02316786 0.11158569]\n",
            "[0.19762917 0.07706193 0.07996851 0.11503758 0.06251268 0.27845099\n",
            " 0.11434353 0.07499562]\n",
            "[0.13051701 0.22577619 0.09015443 0.13675433 0.20161208 0.00332047\n",
            " 0.0449636  0.16690188]\n",
            "[0.15259648 0.10584832 0.14368278 0.00341338 0.03251808 0.21666211\n",
            " 0.24542711 0.09985173]\n",
            "[0.1951612  0.13643378 0.08711413 0.25124369 0.12346201 0.06583885\n",
            " 0.0452318  0.09551455]\n",
            "[0.00323271 0.1679738  0.23440281 0.15129123 0.01741371 0.20974076\n",
            " 0.10902704 0.10691794]\n",
            "[0.04267823 0.12364615 0.14667934 0.14634902 0.20908041 0.08639047\n",
            " 0.15954422 0.08563216]\n",
            "[0.10165745 0.16943696 0.04573778 0.1898676  0.13707382 0.04416968\n",
            " 0.20187002 0.11018669]\n",
            "[0.22209097 0.16767335 0.04205755 0.17504291 0.20292809 0.05767003\n",
            " 0.0141556  0.1183815 ]\n",
            "[0.06727289 0.19594835 0.29309166 0.11188353 0.05435085 0.12439463\n",
            " 0.08665711 0.06640099]\n",
            "[0.08000603 0.19430133 0.15935969 0.02785364 0.14734313 0.14719414\n",
            " 0.0619156  0.18202645]\n",
            "[0.02637385 0.05580666 0.01571174 0.4455988  0.09247855 0.12164294\n",
            " 0.20579923 0.03658823]\n",
            "[0.22352998 0.19037505 0.04621287 0.06830837 0.18253216 0.1215369\n",
            " 0.12561296 0.04189172]\n",
            "[0.00468487 0.09504708 0.25232292 0.06564781 0.22576644 0.00797415\n",
            " 0.12054481 0.22801191]\n",
            "Entrenamiento terminado\n",
            "[0.185 0.152 0.137 0.125 0.116 0.107 0.096 0.082]\n",
            "1.0\n",
            "[0.43810818 0.37072307 0.45121618 0.39189173 0.49265854 0.49911014\n",
            " 0.60048571 0.59214629]\n",
            "En el peor de los casos el retorno seriade 574349.9999999999\n",
            "el retorno maximo posible seria de 1024580.0000000002\n",
            "[185000. 152000. 137000. 125000. 116000. 107000.  96000.  82000.]\n",
            "1000000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No serÃ­a rentable realizar esta inversiÃ³n debido al que el riesgo de pÃ©rdida frente a la baja posibilidad de ganancia siendo mÃ¡s probable perder dinero que ganarlo ya que solo se puede dar uno de los casos a la vez por lo que al distribuir el dinero entre los posibles resultados con el fin de obtener una mayor ganancia o una ganancia segura. Se estÃ¡ perdiendo dinero esto se puede ver mejor si analizamos el mejor de los casos en retorno que es de 1024580 lo que da como margen de ganancia neto tan solo 24580 pesos sobre el 1000000 inicialmente invertido.  "
      ],
      "metadata": {
        "id": "xIhhjB0BGYgB"
      }
    }
  ]
}